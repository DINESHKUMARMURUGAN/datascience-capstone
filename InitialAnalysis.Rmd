---
title: "Initial Analysis"
author: "Dineshkumar Murugan"
date: "Sunday, July 26, 2015"
output: html_document
---

##Goal
The goal of this project is just to display that you've gotten used to working with the data and that you are on track to create your prediction algorithm. Please submit a report on R Pubs (http://rpubs.com/) that explains your exploratory analysis and your goals for the eventual app and algorithm. This document should be concise and explain only the major features of the data you have identified and briefly summarize your plans for creating the prediction algorithm and Shiny app in a way that would be understandable to a non-data scientist manager. You should make use of tables and plots to illustrate important summaries of the data set.




```{r warning=FALSE}
setwd("final/en_US")

file.info(dir())[,1:2]


```

##Basic statistics of the files
Some basics statistics are provided in R by using the stringi package.

```{r warning=FALSE}
library(stringi)
blogs <- stri_read_lines("final/en_US/en_US.blogs.txt")
stri_stats_general(blogs)
```
```{r warning=FALSE}
rm(blogs)
news <- stri_read_lines("final/en_US/en_US.news.txt")
stri_stats_general(news)
rm(news)
```
##Loading a subset of the data
Already from considering a subset of the actual data, it is possible to obtain information on word and bigram frequencies of the whole data set. For this report, we choose a subset of 5000 lines per file.
```{r warning=FALSE}
connection <- file("final/en_US/en_US.twitter.txt", "r", encoding = "UTF-8")
myTwitter <- readLines(connection,5000)
close(connection)
connection <- file("final/en_US/en_US.blogs.txt", "r", encoding = "UTF-8")
myBlogs <- readLines(connection,5000)
close(connection)
connection <- file("final/en_US/en_US.news.txt", "r", encoding = "UTF-8")
myNews <- readLines(connection,5000)
close(connection)
```
##Pre-processing
The tm package provides a very useful set of functions for text mining. After copying the files in one corpus, i.e. into one object database for text files, we perform some basic pre-processing steps for cleaning the files. For the moment, we choose to remove punctuations, to transform all content to lower cases and strip the lines of texts of any additional white spaces. We explicitly decide to leave numbers and stopwords, since they contain typical input when typing a text.

```{r warning=FALSE}
library(NLP)
library(tm)
myCorpus <- Corpus(VectorSource(myTwitter))  
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
myCorpus <- tm_map(myCorpus, stripWhitespace) 
```
The same steps are performed for all three files.
##Exploratory Analysis
In order to get familiar with the data, we transform each text file into matrices of frequent terms and bigrams.
```{r}
tdm <- TermDocumentMatrix(myCorpus)
freq <- rowSums(as.matrix(tdm))
ordered <- order(freq)
freq[tail(ordered,n=10)]
```
The result for the most frequent terms is as expected consisting mostly of stopwords.
```{r}
BigramTokenizer <- function(x)
      unlist(lapply(ngrams(words(x), 2), paste, "", collapse = " "), use.names = FALSE)
tdm2 <- TermDocumentMatrix(myCorpus, control = list(tokenize = BigramTokenizer))
freq <- rowSums(as.matrix(tdm2))
wf <- data.frame(word=names(freq), freq=freq)
```



##Creating a prediction algorithm
In order to avoid memory and performance problems, I intend to do the pre-processing with python and to perform the final analysis in R. In order to get rid of misspelled words and wrong encodings, I will try to remove sparse bigrams and words that contain letters more than three times. In addition, a profanity dictionary will be used to remove the corresponding words.